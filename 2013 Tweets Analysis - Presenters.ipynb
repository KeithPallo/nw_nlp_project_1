{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "import nltk\n",
    "from nltk.probability import FreqDist\n",
    "from nltk.tokenize import TweetTokenizer\n",
    "from nltk.tokenize import sent_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "import string\n",
    "import math\n",
    "import re\n",
    "from collections import Counter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_json('gg2013.json')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_df = pd.DataFrame(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# return cleaned Tweet as string\n",
    "# remove stopwords, user handles, punctuation, urls\n",
    "\n",
    "def cleanTweets(tweet):\n",
    "    tt = TweetTokenizer(strip_handles=True, reduce_len=True, preserve_case=True)\n",
    "\n",
    "    punctuation = list(string.punctuation)\n",
    "    \n",
    "    # strip stopwords, punctuation, url components \n",
    "    stop = stopwords.words('english') + punctuation + ['t.co', 'http', 'https', '...', '..', ':\\\\', 'RT', '#']\n",
    "\n",
    "    strip_nums = re.sub(\"\\d+\", \"\", tweet)\n",
    "    tokenized = tt.tokenize(strip_nums)\n",
    "    terms_stop = [term for term in tokenized if term not in stop]\n",
    "    cleaned = [term for term in terms_stop]\n",
    "    cleaned = ' '.join(cleaned)\n",
    "    \n",
    "    return cleaned"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "data['text'] = data['text'].map(cleanTweets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def findPresenters(data):\n",
    "    include_tweets = ['present', 'presents', 'presenter', 'presenters', 'presenting', 'presented']\n",
    "    need_terms = ['animated']\n",
    "    remove_tweets = []\n",
    "    remove_terms = ['#GoldenGlobes', 'Golden Globes', 'golden globes', '#GoldenGlobe', 'Golden Globe', '#goldenglobes']\n",
    "    \n",
    "    presenter = [];\n",
    "\n",
    "    for tweet in data['text']:\n",
    "        if any(term in tweet for term in include_tweets) and all(term not in tweet for term in remove_tweets) and all(term in tweet for term in need_terms):\n",
    "            for term in remove_terms:\n",
    "                tweet = re.sub(term, \"\", tweet)\n",
    "                presenter.append(tweet)            \n",
    "            \n",
    "    bgrams = [];\n",
    "\n",
    "    for tweet in presenter:\n",
    "        tweet = re.findall('([A-Z][a-z]+)', tweet)\n",
    "        if tweet:\n",
    "            bgrams += list(nltk.bigrams(tweet))\n",
    "        \n",
    "    fdist = nltk.FreqDist(bgrams)\n",
    "    \n",
    "    return fdist.most_common(20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(('Sacha', 'Baron'), 12), (('Baron', 'Cohen'), 12), (('George', 'Bush'), 6), (('What', 'Great'), 6), (('Cohen', 'Tarantino'), 6), (('Cohen', 'Brave'), 6)]\n"
     ]
    }
   ],
   "source": [
    "print(findPresenters(data))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

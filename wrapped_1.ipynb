{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Preprocessing Start"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package wordnet to /Users/yqiao/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "import nltk\n",
    "from nltk.probability import FreqDist\n",
    "from nltk.tokenize import TweetTokenizer\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.corpus import wordnet\n",
    "nltk.download('wordnet')\n",
    "\n",
    "from collections import Counter\n",
    "from collections import OrderedDict\n",
    "\n",
    "import string\n",
    "import re\n",
    "import unidecode\n",
    "import requests\n",
    "import json\n",
    "from pprint import pprint"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Begin Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "OFFICIAL_AWARDS_1315 = ['cecil b. demille award', 'best motion picture - drama', 'best performance by an actress in a motion picture - drama', 'best performance by an actor in a motion picture - drama', 'best motion picture - comedy or musical', 'best performance by an actress in a motion picture - comedy or musical', 'best performance by an actor in a motion picture - comedy or musical', 'best animated feature film', 'best foreign language film', 'best performance by an actress in a supporting role in a motion picture', 'best performance by an actor in a supporting role in a motion picture', 'best director - motion picture', 'best screenplay - motion picture', 'best original score - motion picture', 'best original song - motion picture', 'best television series - drama', 'best performance by an actress in a television series - drama', 'best performance by an actor in a television series - drama', 'best television series - comedy or musical', 'best performance by an actress in a television series - comedy or musical', 'best performance by an actor in a television series - comedy or musical', 'best mini-series or motion picture made for television', 'best performance by an actress in a mini-series or motion picture made for television', 'best performance by an actor in a mini-series or motion picture made for television', 'best performance by an actress in a supporting role in a series, mini-series or motion picture made for television', 'best performance by an actor in a supporting role in a series, mini-series or motion picture made for television']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load in official award names\n",
    "\n",
    "OFFICIAL_AWARDS_1315_MEDIA = ['best motion picture - drama', \n",
    "                              'best motion picture - comedy or musical', \n",
    "                              'best animated feature film', \n",
    "                              'best foreign language film', \n",
    "                              'best screenplay - motion picture', \n",
    "                              'best original score - motion picture', \n",
    "                              'best original song - motion picture', \n",
    "                              'best television series - drama', \n",
    "                              'best television series - comedy or musical', \n",
    "                              'best mini-series or motion picture made for television']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "OFFICIAL_AWARDS_1315_PEOPLE = ['cecil b. demille award',\n",
    "                        'best performance by an actress in a motion picture - drama', \n",
    "                        'best performance by an actor in a motion picture - drama',\n",
    "                        'best performance by an actress in a motion picture - comedy or musical', \n",
    "                        'best performance by an actor in a motion picture - comedy or musical',\n",
    "                        'best performance by an actress in a supporting role in a motion picture', \n",
    "                        'best performance by an actor in a supporting role in a motion picture', \n",
    "                        'best director - motion picture', \n",
    "                        'best performance by an actress in a television series - drama', \n",
    "                        'best performance by an actor in a television series - drama',\n",
    "                        'best performance by an actress in a television series - comedy or musical', \n",
    "                        'best performance by an actor in a television series - comedy or musical', \n",
    "                        'best performance by an actress in a mini-series or motion picture made for television', \n",
    "                        'best performance by an actor in a mini-series or motion picture made for television', \n",
    "                        'best performance by an actress in a supporting role in a series, mini-series or motion picture made for television', \n",
    "                        'best performance by an actor in a supporting role in a series, mini-series or motion picture made for television']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load in answers - we will remove this when we submit the final fial\n",
    "\n",
    "with open('gg2015answers.json') as f:\n",
    "    answers = json.load(f)\n",
    "    \n",
    "true_dict = answers['award_data'] # use this for true dict in the below function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loadin dataset - TO DO, automate this processes for four relevant years (2013,2015,2018,2019)\n",
    "\n",
    "df = pd.read_json('../gg2015.json')\n",
    "df = pd.DataFrame(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Write dataset to a list\n",
    "\n",
    "data = df['text'].values.tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "f = open('english.txt', 'r')\n",
    "stop_words = f.read().splitlines()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Begin querying knowledge bases"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### People KB Creation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize seperate knowledge bases \n",
    "\n",
    "people = set()\n",
    "media = set() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "url = 'https://query.wikidata.org/sparql'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "query = \"\"\"\n",
    "# ALL PERSONS required for awards\n",
    "SELECT DISTINCT ?person ?personLabel WHERE {\n",
    "# FIRST: uncomment occupation:\n",
    "  ?person wdt:P31 wd:Q5;\n",
    "           wdt:P106/wdt:P279* wd:Q2526255; #uncomment for     FILM director (no award for TV director)\n",
    "  FILTER NOT EXISTS { ?person wdt:P570 ?date. } #person is alive\n",
    "  \n",
    "# SECOND: uncomment gender if applicable (for actor/actress):\n",
    "#          wdt:P21 wd:Q6581097;    #male\n",
    "#          wdt:P21 wd:Q6581072;    #female\n",
    "  SERVICE wikibase:label { bd:serviceParam wikibase:language \"[AUTO_LANGUAGE],en\". }\n",
    "}\n",
    "\"\"\"\n",
    "r = requests.get(url, params = {'format': 'json', 'query': query})\n",
    "kb = r.json()\n",
    "for item in kb['results']['bindings']:\n",
    "    people.add(unidecode.unidecode(item['personLabel']['value']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "query = \"\"\"\n",
    "# ALL PERSONS required for awards\n",
    "SELECT DISTINCT ?person ?personLabel WHERE {\n",
    "# FIRST: uncomment occupation:\n",
    "  ?person wdt:P31 wd:Q5;\n",
    "           wdt:P106/wdt:P279* wd:Q10800557; #uncomment for    FILM actor (don't just use actor)\n",
    "  FILTER NOT EXISTS { ?person wdt:P570 ?date. } #person is alive\n",
    "  \n",
    "# SECOND: uncomment gender if applicable (for actor/actress):\n",
    "#          wdt:P21 wd:Q6581097;    #male\n",
    "#          wdt:P21 wd:Q6581072;    #female\n",
    "  SERVICE wikibase:label { bd:serviceParam wikibase:language \"[AUTO_LANGUAGE],en\". }\n",
    "}\n",
    "\"\"\"\n",
    "r1 = requests.get(url, params = {'format': 'json', 'query': query})\n",
    "kb1 = r1.json()\n",
    "for item in kb1['results']['bindings']:\n",
    "    people.add(unidecode.unidecode(item['personLabel']['value']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "query = \"\"\"\n",
    "# ALL PERSONS required for awards\n",
    "SELECT DISTINCT ?person ?personLabel WHERE {\n",
    "# FIRST: uncomment occupation:\n",
    "  ?person wdt:P31 wd:Q5;\n",
    "           wdt:P106/wdt:P279* wd:Q10798782; #uncomment for    TV actor (don't just use actor)\n",
    "  FILTER NOT EXISTS { ?person wdt:P570 ?date. } #person is alive\n",
    "  \n",
    "# SECOND: uncomment gender if applicable (for actor/actress):\n",
    "#          wdt:P21 wd:Q6581097;    #male\n",
    "#          wdt:P21 wd:Q6581072;    #female\n",
    "  SERVICE wikibase:label { bd:serviceParam wikibase:language \"[AUTO_LANGUAGE],en\". }\n",
    "}\n",
    "\"\"\"\n",
    "r = requests.get(url, params = {'format': 'json', 'query': query})\n",
    "kb = r.json()\n",
    "for item in kb['results']['bindings']:\n",
    "    people.add(unidecode.unidecode(item['personLabel']['value']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "query = \"\"\"\n",
    "# ALL PERSONS required for awards\n",
    "SELECT DISTINCT ?person ?personLabel WHERE {\n",
    "# FIRST: uncomment occupation:\n",
    "  ?person wdt:P31 wd:Q5;\n",
    "           wdt:P106/wdt:P279* wd:Q36834; #uncomment for       composer (cannot use songwriter)\n",
    "  FILTER NOT EXISTS { ?person wdt:P570 ?date. } #person is alive\n",
    "  \n",
    "# SECOND: uncomment gender if applicable (for actor/actress):\n",
    "#          wdt:P21 wd:Q6581097;    #male\n",
    "#          wdt:P21 wd:Q6581072;    #female\n",
    "  SERVICE wikibase:label { bd:serviceParam wikibase:language \"[AUTO_LANGUAGE],en\". }\n",
    "}\n",
    "\"\"\"\n",
    "r = requests.get(url, params = {'format': 'json', 'query': query})\n",
    "kb = r.json()\n",
    "for item in kb['results']['bindings']:\n",
    "    people.add(unidecode.unidecode(item['personLabel']['value']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "query = \"\"\"\n",
    "# ALL PERSONS required for awards\n",
    "SELECT DISTINCT ?person ?personLabel WHERE {\n",
    "# FIRST: uncomment occupation:\n",
    "  ?person wdt:P31 wd:Q5;\n",
    "           wdt:P106/wdt:P279* wd:Q28389; #uncomment for       screenwriter\n",
    "  FILTER NOT EXISTS { ?person wdt:P570 ?date. } #person is alive\n",
    "  \n",
    "# SECOND: uncomment gender if applicable (for actor/actress):\n",
    "#          wdt:P21 wd:Q6581097;    #male\n",
    "#          wdt:P21 wd:Q6581072;    #female\n",
    "  SERVICE wikibase:label { bd:serviceParam wikibase:language \"[AUTO_LANGUAGE],en\". }\n",
    "}\n",
    "\"\"\"\n",
    "r = requests.get(url, params = {'format': 'json', 'query': query})\n",
    "kb = r.json()\n",
    "for item in kb['results']['bindings']:\n",
    "    people.add(unidecode.unidecode(item['personLabel']['value']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "query = \"\"\"\n",
    "# ALL PERSONS required for awards\n",
    "SELECT DISTINCT ?person ?personLabel WHERE {\n",
    "# FIRST: uncomment occupation:\n",
    "  ?person wdt:P31 wd:Q5;\n",
    "          wdt:P106/wdt:P279* wd:Q177220; #uncomment for       singer\n",
    "  FILTER NOT EXISTS { ?person wdt:P570 ?date. } #person is alive\n",
    "  \n",
    "# SECOND: uncomment gender if applicable (for actor/actress):\n",
    "#          wdt:P21 wd:Q6581097;    #male\n",
    "#          wdt:P21 wd:Q6581072;    #female\n",
    "  SERVICE wikibase:label { bd:serviceParam wikibase:language \"[AUTO_LANGUAGE],en\". }\n",
    "}\n",
    "\"\"\"\n",
    "r = requests.get(url, params = {'format': 'json', 'query': query})\n",
    "kb = r.json()\n",
    "for item in kb['results']['bindings']:\n",
    "    people.add(unidecode.unidecode(item['personLabel']['value']))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Media KB Creation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "url = 'https://query.wikidata.org/sparql'\n",
    "query = \"\"\"\n",
    "SELECT DISTINCT ?itemLabel  WHERE {\n",
    " ?item wdt:P31 wd:Q11424. ?item wdt:P577 ?_publication_date. ?item wdt:P136 ?_genre.\n",
    " ?_genre rdfs:label ?_genreLabel. BIND(str(YEAR(?_publication_date)) AS ?year)\n",
    " FILTER((LANG(?_genreLabel)) = \"en\")\n",
    " FILTER (?_publication_date >= \"2012-00-00T00:00:00Z\"^^xsd:dateTime && ?_publication_date <= \"2019-00-00T00:00:00Z\"^^xsd:dateTime )\n",
    " SERVICE wikibase:label { bd:serviceParam wikibase:language \"[AUTO_LANGUAGE],en\" .} }\n",
    "\"\"\"\n",
    "r = requests.get(url, params = {'format': 'json', 'query': query})\n",
    "kb_m = r.json()\n",
    "\n",
    "for item in kb_m['results']['bindings']:\n",
    "    media.add(unidecode.unidecode(item['itemLabel']['value']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "query = \"\"\"\n",
    "SELECT DISTINCT ?itemLabel  WHERE {\n",
    "  ?item wdt:P31 wd:Q5398426.\n",
    "  ?item wdt:P580  ?_start\n",
    " FILTER (?_start >= \"2005-00-00T00:00:00Z\"^^xsd:dateTime && ?_start <= \"2019-00-00T00:00:00Z\"^^xsd:dateTime )\n",
    "  SERVICE wikibase:label {bd:serviceParam wikibase:language \"[AUTO_LANGUAGE],en\" .}\n",
    "}\n",
    "    \n",
    "\"\"\"\n",
    "r = requests.get(url, params = {'format': 'json', 'query': query})\n",
    "kb_m = r.json()\n",
    "\n",
    "for item in kb_m['results']['bindings']:\n",
    "    media.add(unidecode.unidecode(item['itemLabel']['value']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "people = set(map(lambda x: x.lower(),people))\n",
    "media = set(map(lambda x: x.lower(),media)) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Preprocessing Complete\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Helper Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "def separate_awards(award_list):\n",
    "    possible_people_awards = ['actor',' actress', 'musician', ' singer', 'composer', 'director', 'producer',\n",
    "                        'screenwriter', 'stage technician', 'author']\n",
    "\n",
    "    people_awards = []\n",
    "    media_awards = []\n",
    "\n",
    "    for category in award_list:\n",
    "        if any(job in category.lower() for job in possible_people_awards):\n",
    "            people_awards.append(category)\n",
    "        else:\n",
    "            media_awards.append(category)\n",
    "\n",
    "    return people_awards, media_awards"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "def parse_award(award):\n",
    "    \"\"\"\n",
    "    Returns a list of words that can be used to filter for a particular award\n",
    "    \"\"\"\n",
    "    \n",
    "    award = re.split('\\W+', award)\n",
    "    award = [i for i in award if i not in stop_words]\n",
    "    award = list(set(award))\n",
    "    return award"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_awards_dict(awards_list):\n",
    "    \"\"\"\n",
    "    Returns a dictionary that has all awards as keys, and a list of relevant filtering words as values\n",
    "    \"\"\"\n",
    "    \n",
    "    categories_dict = dict()\n",
    "    for a in awards_list:\n",
    "        terms = parse_award(a)\n",
    "        categories_dict[a] = terms\n",
    "\n",
    "    return categories_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_all_awards_tweets(award_list, categories_dict, data):\n",
    "    \"\"\"\n",
    "    Using an award list and category dictionary, filters out tweets at an award level\n",
    "    \"\"\"\n",
    "    \n",
    "    d = {}\n",
    "    for award in award_list:\n",
    "        d[\"{0}\".format(award)] = get_award_tweets(data, categories_dict[award])\n",
    "                                         \n",
    "    return d"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_award_tweets(data, list1, spec = \"people\"):\n",
    "    \"\"\"\n",
    "    Returns a list of tweets that are relevant to a particular award\n",
    "    \"\"\"\n",
    "    synonyms = {}\n",
    "    \n",
    "    if spec == \"people\":\n",
    "        synonyms = {\n",
    "            'motion' : ['motion picture', 'motion', 'picture', 'movie'],\n",
    "            'picture' : ['motion picture', 'motion', 'picture', 'movie'],\n",
    "            'television' : ['television', 'tv'],\n",
    "            'mini' : ['mini-series', 'mini', 'series', 'miniseries'],\n",
    "            'series' : ['mini-series', 'mini', 'series', 'miniseries']\n",
    "        }\n",
    "\n",
    "    \n",
    "    result = []\n",
    "    \n",
    "    list1 = [i for i in list1 if i != 'performance' and i != 'role']\n",
    "\n",
    "    for tweet in data:\n",
    "        cond = True\n",
    "        for i in list1:\n",
    "            if i in synonyms:\n",
    "                if all(j not in tweet.lower() for j in synonyms[i]):\n",
    "                    cond = False\n",
    "            elif i not in tweet.lower():\n",
    "                cond = False\n",
    "        if cond:\n",
    "            result.append(tweet)\n",
    "\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compare_to_kb(nominees,kb):\n",
    "    \"\"\"\n",
    "    Takes in a dictionary of potential nominees and removes those that don't appear in a relevant KB\n",
    "    \n",
    "    If no nominees are in the KB, then ... (currentlt top five)\n",
    "    \"\"\"\n",
    "    \n",
    "    final_nominees = {}\n",
    "        \n",
    "    for i in nominees:\n",
    "        award_nominees = []\n",
    "        \n",
    "        for j in nominees[i]:\n",
    "            #print(j)\n",
    "            if j[0].lower() in kb:\n",
    "                award_nominees.append(j[0].lower())\n",
    "                \n",
    "        if not award_nominees:\n",
    "            award_nominees = [i[0].lower() for i in nominees[i][:5]]\n",
    "        \n",
    "        award_nominees = list(set(award_nominees))\n",
    "        final_nominees[i] = award_nominees\n",
    "        \n",
    "    return final_nominees"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Award Categories Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_awards(text):\n",
    "    \" Cleans individual tweet for award search\"\n",
    "    \n",
    "    remove_terms = ['#goldenglobes', 'golden globes', '#goldenglobe', 'golden globe', 'goldenglobes', 'goldenglobe', 'rt', 'golden', 'globe', 'globes']\n",
    "    \n",
    "    text = re.sub(\"(\\s)#\\w+\",\"\",text)    # strips away all hashtags \n",
    "    text = re.sub(\"RT\",\"\",text)          # removes retweet\n",
    "    text = re.sub(\"[^a-zA-Z ]\", '',text) # removes all punctuation but keeps whitespace for tokenization\n",
    "    text = text.lower()\n",
    "    text = text.split()\n",
    "    text = \" \".join([term for term in text if term not in remove_terms]) #remove stop words\n",
    "    \n",
    "    return text "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_tags(tweet):\n",
    "    \"\"\"\n",
    "    Performs pos tagging at a tweet level\n",
    "    \"\"\"\n",
    "    \n",
    "    tokenizer = TweetTokenizer()\n",
    "    tokens = tokenizer.tokenize(tweet)\n",
    "    tags = nltk.pos_tag(tokens)\n",
    "    return tags"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pos_search(tags,chunk_gram,label):\n",
    "    \"\"\n",
    "    \n",
    "    \n",
    "    potentials = \"\"\n",
    "    chunk_parser = nltk.RegexpParser(chunk_gram)\n",
    "    chunked = chunk_parser.parse(tags)\n",
    "    for subtree in chunked.subtrees():\n",
    "        if subtree.label() == label: \n",
    "            raw_list = untag(subtree)\n",
    "            raw_list = [i for i in raw_list if wordnet.synsets(i)]\n",
    "            string = ' '.join(raw_list)\n",
    "            if \"best\" in string[0:6]:\n",
    "                if len(string) >= len(potentials):\n",
    "                    potentials = string\n",
    "                    \n",
    "    if potentials == \"\":\n",
    "        return \"No Chunk\"\n",
    "\n",
    "    return potentials"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "def filter_df(df,label):\n",
    "    \n",
    "    data = df.loc[df[label] != \"No Chunk\"]\n",
    "    data.drop(data.columns.difference([label]), 1, inplace=True)\n",
    "    single_list = list(data[label])\n",
    "    freq = FreqDist(single_list)\n",
    "    \n",
    "    return data, freq"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_awards(df):\n",
    "    \"\"\"\n",
    "    Returns a list of strings for all possible awards\n",
    "    \"\"\"\n",
    "    # Shuffle data if necesarry\n",
    "    sample_size = 200000\n",
    "    if len(df['text']) > sample_size:\n",
    "        df = df.sample(n=sample_size)\n",
    "    \n",
    "    # Clean awards, keep best, pos tag\n",
    "    df['text'] = df['text'].apply(lambda x:  clean_awards(x))\n",
    "    df_a = df[df['text'].str.contains(\"best\")]\n",
    "    df_a['tags'] = df_a['text'].apply(lambda x: find_tags(x))\n",
    "    \n",
    "    # Define regex patterns from generalized \n",
    "    regex_pattern_0 = \"P0: {<JJ.><NN.|JJ|VBG><...?>*<NN.>}\"\n",
    "    regex_pattern_1 = \"P1: {<NN.><IN|NN.|IN><...?>*<NN.>}\"\n",
    "    regex_pattern_2 = \"P2: {<RB.><JJ|NN.|VGB><...?>*<NN.|JJ>}\"\n",
    "    \n",
    "    # Search for pos \n",
    "    df_a['chunks_0'] = df_a['tags'].apply(lambda x: pos_search(x,regex_pattern_0,\"P0\"))\n",
    "    df_a['chunks_1'] = df_a['tags'].apply(lambda x: pos_search(x,regex_pattern_1,\"P1\"))\n",
    "    df_a['chunks_2'] = df_a['tags'].apply(lambda x: pos_search(x,regex_pattern_2,\"P2\"))\n",
    "\n",
    "    data_0, freq_0 = filter_df(df_a,\"chunks_0\")\n",
    "    data_1, freq_1 = filter_df(df_a,\"chunks_1\")\n",
    "    data_2, freq_2 = filter_df(df_a,\"chunks_2\")\n",
    "\n",
    "    freq = freq_0 + freq_1 + freq_2\n",
    "    \n",
    "    possible = []\n",
    "\n",
    "    for i in freq.most_common():\n",
    "        if i[1] >= 8: possible.append(i[0])\n",
    "    \n",
    "    return possible"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Hosts Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "# DATA PASSED IN AS LIST\n",
    "def extract_hosts(data):\n",
    "   # clean data\n",
    "    cleaned_data = []\n",
    "\n",
    "    for tweet in data:\n",
    "        tt = TweetTokenizer(strip_handles=True, reduce_len=True, preserve_case=True)\n",
    "\n",
    "        punctuation = list(string.punctuation)\n",
    "\n",
    "        # strip stopwords, punctuation, url components\n",
    "        stop = stopwords.words('english') + punctuation + ['t.co', 'http', 'https', '...', '..', ':\\\\', 'RT', '#']\n",
    "\n",
    "        strip_nums = re.sub(\"\\d+\", \"\", tweet)\n",
    "        tokenized = tt.tokenize(strip_nums)\n",
    "        terms_stop = [term for term in tokenized if term not in stop]\n",
    "        cleaned = [term for term in terms_stop]\n",
    "        cleaned = ' '.join(cleaned)\n",
    "        cleaned_data.append(cleaned)\n",
    "\n",
    "\n",
    "    # find host\n",
    "    include_terms = ['host', 'hosted', 'hosting', 'hosts']\n",
    "    remove_terms = ['next year']\n",
    "    host = [];\n",
    "    cohost = False;\n",
    "\n",
    "    for tweet in cleaned_data:\n",
    "        if any(term in tweet for term in include_terms) and any(term not in tweet for term in remove_terms):\n",
    "            host.append(tweet)\n",
    "        if 'cohost' in tweet:\n",
    "            cohost = True\n",
    "\n",
    "    bgrams = [];\n",
    "\n",
    "    for tweet in host:\n",
    "        bgrams += list(nltk.bigrams(tweet.split()))\n",
    "\n",
    "    fdist = nltk.FreqDist(bgrams)\n",
    "\n",
    "    if cohost:\n",
    "        fdist = fdist.most_common(2)\n",
    "    else:\n",
    "        fdist = fdist.most_common(1)\n",
    "\n",
    "    final_hosts = []\n",
    "    for host in fdist:\n",
    "        name = host[0][0] + ' ' + host[0][1]\n",
    "        final_hosts.append(name)\n",
    "\n",
    "    return final_hosts"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Extract Functions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Extract all instances of [people / media / presenters] from tweets for specific award"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def extract_people(data, list1):\n",
    "    \"\"\"\n",
    "    Extracts potential People nominees from an individual tweet\n",
    "    \"\"\"\n",
    "        \n",
    "    result = []\n",
    "       \n",
    "    translator = str.maketrans('', '', string.punctuation)\n",
    "    remove_terms = ['#goldenglobes', 'golden globes', '#goldenglobe', 'golden globe', 'goldenglobes', 'goldenglobe', 'golden', 'globe', 'globes']\n",
    "    stop = remove_terms + list1\n",
    "    \n",
    "    for tweet in data:\n",
    "        \n",
    "        tweet = re.sub(\"\\d+\", \"\", tweet)       #strip nums\n",
    "        tweet = re.sub(r'http\\S+', '', tweet)  #strip urls\n",
    "        tweet = re.sub(r'#\\S+', '', tweet)     #strip hashtags\n",
    "        tweet = tweet.translate(translator)    #strip non-alphanumeric characters\n",
    "        tweet = tweet.split()                  #tokenize\n",
    "        tweet = [term for term in tweet if term.lower() not in stop_words] #remove stop words\n",
    "        for i in stop:\n",
    "            for j in tweet:\n",
    "                if i.lower() in j.lower():\n",
    "                    tweet.remove(j)\n",
    "        result.append(tweet)\n",
    "        \n",
    "\n",
    "        \n",
    "    grams = [];\n",
    "\n",
    "    for tweet in result:\n",
    "        if tweet:\n",
    "            # Get all possible bigrams & trigrams in a tweet\n",
    "            gram = list(nltk.everygrams(tweet, 2, 3))\n",
    "            \n",
    "            # Filter through and append to list for tweet\n",
    "            for g in gram:\n",
    "                if len(g) == 2:\n",
    "                    if bool(re.match(r'\\b[A-Z][a-z]+\\b', g[0])) and bool(re.match(r'\\b[A-Z][a-z]+\\b', g[1])):\n",
    "                        grams.append(' '.join(g))\n",
    "                else:\n",
    "                    if bool(re.match(r'\\b[A-Z][a-z]+\\b', g[0])) and bool(re.match(r'\\b[A-Z][a-z]+\\b', g[1])) and bool(re.match(r'\\b[A-Z][a-z]+\\b', g[2])):\n",
    "                        grams.append(' '.join(g))\n",
    "\n",
    "  \n",
    "    fdist = nltk.FreqDist(grams)\n",
    "\n",
    "    try:\n",
    "        names = fdist.most_common()\n",
    "    except:\n",
    "        names = \"nothing here\"\n",
    "    \n",
    "    return names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_media(data, list1):\n",
    "    \"\"\"\n",
    "    Extracts potential media nominees from an individual tweet\n",
    "    \"\"\"\n",
    "    \n",
    "    result = []\n",
    "       \n",
    "    translator = str.maketrans('', '', string.punctuation)\n",
    "    remove_terms = ['#goldenglobes', 'golden globes', '#goldenglobe', 'golden globe', 'goldenglobes', 'goldenglobe', 'golden', 'globe', 'globes', 'best']    \n",
    "    stop = remove_terms + list1\n",
    "    \n",
    "    for tweet in data:\n",
    "        tweet = re.sub(\"\\d+\", \"\", tweet)      #strip nums\n",
    "        tweet = re.sub(r'http\\S+', '', tweet) #strip urls\n",
    "        tweet = re.sub(r'#\\S+', '', tweet)    #strip hashtags\n",
    "        tweet = tweet.translate(translator)   #strip non-alphanumeric characters\n",
    "        tweet = tweet.split()                 #tokenize\n",
    "        for i in stop:\n",
    "            for j in tweet:\n",
    "                if i.lower() in j.lower():\n",
    "                    tweet.remove(j)\n",
    "        tweet = ' '.join(tweet)\n",
    "        result.append(tweet)\n",
    "        \n",
    "        \n",
    "    grams = [];\n",
    "\n",
    "    for tweet in result:\n",
    "        if tweet:\n",
    "            \n",
    "            grams.extend(re.findall(r\"([A-Z][\\w-]*(?:\\s+[A-Z][\\w-]*)+)\", tweet))\n",
    "            grams.extend(re.findall(r\"\\b[A-Z][a-z]+\\b.*\\b[A-Z][a-z]+\\b\", tweet))\n",
    "            #singular = re.findall(r\"\\b[A-Z][a-z]+\\b\", tweet)\n",
    "            #singular = [i for i in singular if not wordnet.synsets(i)]\n",
    "            #grams.extend(singular)\n",
    "            \n",
    "    # print(grams)\n",
    "    fdist = nltk.FreqDist(grams)\n",
    "\n",
    "    try:\n",
    "        names = fdist.most_common()\n",
    "\n",
    "    except:\n",
    "        names = \"nothing here\"\n",
    "    \n",
    "    return names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_presenters(data, list1, winners):\n",
    "    #print(data[0])\n",
    "    result = []\n",
    "       \n",
    "    #tt = TweetTokenizer(strip_handles=True, reduce_len=True, preserve_case=True)\n",
    "    translator = str.maketrans('', '', string.punctuation)\n",
    "    remove_terms = ['#goldenglobes', 'golden globes', '#goldenglobe', 'golden globe', 'goldenglobes', 'goldenglobe', 'golden', 'globe', 'globes']\n",
    "    stop = remove_terms + list1 + winners.split()\n",
    "    \n",
    "    for tweet in data:\n",
    "        #print(tweet)\n",
    "        tweet = re.sub(\"\\d+\", \"\", tweet) #strip nums\n",
    "        tweet = re.sub(r'http\\S+', '', tweet) #strip urls\n",
    "        tweet = re.sub(r'#\\S+', '', tweet) #strip hashtags\n",
    "        tweet = tweet.translate(translator) #strip non-alphanumeric characters\n",
    "        tweet = tweet.split() #tokenize\n",
    "        #tweet = [term for term in tweet if term.lower() not in stop_words] #remove stop words\n",
    "        for i in stop:\n",
    "            for j in tweet:\n",
    "                if i.lower() in j.lower():\n",
    "                    tweet.remove(j)\n",
    "        result.append(tweet)\n",
    "        \n",
    "    #print(result[:20])\n",
    "        \n",
    "    grams = [];\n",
    "\n",
    "    for tweet in result:\n",
    "        if tweet:\n",
    "            gram = list(nltk.everygrams(tweet, 2, 3))\n",
    "            #print(bigram[:10])\n",
    "            for g in gram:\n",
    "                if len(g) == 2:\n",
    "                    if bool(re.match(r'\\b[A-Z][a-z]+\\b', g[0])) and bool(re.match(r'\\b[A-Z][a-z]+\\b', g[1])):\n",
    "                        grams.append(' '.join(g))\n",
    "                else:\n",
    "                    if bool(re.match(r'\\b[A-Z][a-z]+\\b', g[0])) and bool(re.match(r'\\b[A-Z][a-z]+\\b', g[1])) and bool(re.match(r'\\b[A-Z][a-z]+\\b', g[2])):\n",
    "                        grams.append(' '.join(g))\n",
    "\n",
    "  \n",
    "    fdist = nltk.FreqDist(grams)\n",
    "    #print(fdist)\n",
    "\n",
    "    try:\n",
    "        names = fdist.most_common()\n",
    "        #names = [' '.join(i[0]) for i in fdist.most_common()]\n",
    "    except:\n",
    "        names = \"nothing here\"\n",
    "    \n",
    "    return names"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Get Functions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Get final [nominees / presenters / media / people] results for all desired awards"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_nominees(award_list, categories_dict, tweets_dict, spec = \"\"):\n",
    "    \"\"\"\n",
    "    Gets all potential nominees based on extract_media or extract_people\n",
    "    \"\"\"\n",
    "    \n",
    "    if spec == \"people\":\n",
    "        funct = extract_people\n",
    "    elif spec == \"media\":\n",
    "        funct = extract_media\n",
    "    else:\n",
    "        print(\"there is a problem\")\n",
    "\n",
    "    nominees = {}\n",
    "    for award in award_list:\n",
    "        nominees[\"{0}\".format(award)] = funct(tweets_dict[award], categories_dict[award])\n",
    "    \n",
    "    return nominees"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_presenters(award_list, categories_dict, tweets_dict, winners):\n",
    "    present = ['present', 'annouc', 'introduc']\n",
    "    \n",
    "    for award in award_list:\n",
    "        for tweet in tweets_dict[award]:\n",
    "            if all(i not in tweet for i in present):\n",
    "                tweets_dict[award].remove(tweet)\n",
    "            \n",
    "    presenters = {}\n",
    "    for award in award_list:\n",
    "        all_presenters = extract_presenters(tweets_dict[award], categories_dict[award], winners[award])\n",
    "        #print(all_presenters)\n",
    "        presenters[award] = ' '.join(all_presenters[0][0])\n",
    "    \n",
    "    return presenters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "## To DO\n",
    "\n",
    "def get_media_winners(nominees):\n",
    "    pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_people_winners(nominees):\n",
    "    final_winners = {}\n",
    "    #print(nominees)\n",
    "    \n",
    "    for award in nominees:\n",
    "        winner = ' '.join(nominees[award][0][0])\n",
    "        final_winners[award] = winner\n",
    "        \n",
    "    return final_winners"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Wrapper Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compress_associated_dict(award_list,nominees,winners,presenters):\n",
    "    \n",
    "    our_dict = {}\n",
    "\n",
    "    for award in award_list:\n",
    "        our_dict[award] = {\n",
    "            'nominees' : nominees[award],\n",
    "            'winner' : winners[award],\n",
    "            'presenters' : presenters[award]\n",
    "        }\n",
    "        \n",
    "    return our_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [],
   "source": [
    "def associated_tasks(award_list,data,spec,kb,kb2):\n",
    "    \n",
    "    # Create a dictionary to filter tweets at a category level\n",
    "    cat_filter_dict = get_awards_dict(award_list)                 \n",
    "    #print(cat_filter_dict)\n",
    "    \n",
    "    # Get all associated tweets for each award \n",
    "    tweets_dict = get_all_awards_tweets(award_list, cat_filter_dict, data)\n",
    "    print(tweets_dict)\n",
    "    \n",
    "    # For each award, get all associated nominees\n",
    "    full_nom_dict = get_nominees(award_list, cat_filter_dict, tweets_dict, spec)\n",
    "    #print(full_nom_dict)\n",
    "    \n",
    "    # Filter out all nominees that are not in the dictionary\n",
    "    final_nom = compare_to_kb(full_nom_dict, kb)\n",
    "    \n",
    "    \n",
    "    # TO DO - Fill in the below functions\n",
    "    final_winners = {}\n",
    "    if spec == \"media\":\n",
    "        pass \n",
    "        # final_winners = get_media_winners()\n",
    "    \n",
    "    elif spec == \"people\":\n",
    "        final_winners = get_people_winners(full_nom_dict)\n",
    "    \n",
    "    # Get possible presenters \n",
    "    full_presenters = get_presenters(award_list, cat_filter_dict, tweets_dict, final_winners)\n",
    "    \n",
    "    # Filter out all presenters not in the secondary kb\n",
    "    final_pres = compare_to_kb(full_presenters,kb2)\n",
    "    \n",
    "    # compressed_dict = compress_associated_dict(award_list,final_nom,final_winners,final_pres)\n",
    "    \n",
    "    # This return is only required for main_exec\n",
    "    return tweets_dict, full_nom_dict, final_nom, final_pres"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "def main_exec(award_list,df,kb_p,kb_m):\n",
    "    \"\"\"\n",
    "    Main execution file - how you run the program\n",
    "    Itype: kb_p and kb_m are sets for our built KB's\n",
    "    \"\"\"\n",
    "    \n",
    "    data = df['text'].values.tolist()\n",
    "    \n",
    "    # Call host search function\n",
    "    # // To Do // - Insert function call (should write to JSON / return a variable to write later )\n",
    "    host = extract_hosts(data)\n",
    "    \n",
    "    # Call award recognition function\n",
    "    # // To Do // - Insert function call (should write to JSON / return a variable to write later)\n",
    "    \n",
    "    # Set a variable to the hardcoded list\n",
    "    hardcoded = award_list\n",
    "    \n",
    "    # Segment out awards award categories\n",
    "    people_awards, media_awards = seperateAwards(hardcoded)\n",
    "    \n",
    "    \n",
    "    ## Functions below need to return a dictionary with following structure\n",
    "    ## Key 1: Award Name, Value: Dictionary\n",
    "    ## Key 2: [ Nominees, Winners, Presenters] \n",
    "    \n",
    "    \n",
    "    # Call people award search function - winner, nominee, presenter (potentially swap last two)\n",
    "    \n",
    "    people_tweet, all_potential_people, full_people_dict = associated_tasks(people_awards, data, \"people\", kb_p, kb_p)\n",
    "    \n",
    "    media_tweet, all_potential_media, full_media_dict = associated_tasks(media_awards, data, \"media\", kb_m, kb_p)\n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    # Call people award search function - winner, nominee, presenter (potentially swap last two)\n",
    "    ## // To DO // - Insert function call ( only write to easy_comp now)\n",
    "    \n",
    "    \n",
    "    # Merge dictionaries from two above functions\n",
    "    # Return single dict for easy_comp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "def easy_comp(award_list,true_dict,our_dict):\n",
    "        \n",
    "    # Input \n",
    "    # Dict of dictionaries \n",
    "    # Keys - Award Name\n",
    "    # Values - Dictionary with keys (nominees, presenters, winner)\n",
    "    \n",
    "    # Output\n",
    "    # Nested List (some elements are dictionarys)\n",
    "\n",
    "    output = []\n",
    "\n",
    "    for award in award_list:\n",
    "        output.append([award,[\"Guess\",our_dict[award]],[[\"True\",true_dict[award]]]])\n",
    "\n",
    "    \n",
    "    return output\n",
    "\n",
    "# For easy printing use pprint. It works nicely for updated comparison"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "people_categories_dict = get_awards_dict(OFFICIAL_AWARDS_1315_PEOPLE)\n",
    "media_categories_dict = get_awards_dict(OFFICIAL_AWARDS_1315_MEDIA)\n",
    "media_categories_dict;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "132.44802344599975\n"
     ]
    }
   ],
   "source": [
    "import timeit\n",
    "start_time = timeit.default_timer()\n",
    "tweets = get_all_awards_tweets(OFFICIAL_AWARDS_1315_MEDIA, media_categories_dict,data)\n",
    "elapsed = timeit.default_timer() - start_time\n",
    "print(elapsed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "media_nominees = get_nominees(OFFICIAL_AWARDS_1315_MEDIA, media_categories_dict, tweets, \"media\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "# media_nominees"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "media_nominees_dict = compare_to_kb(media_nominees, media)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    " # media_nominees_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "our_dict = {}\n",
    "\n",
    "for award in OFFICIAL_AWARDS_1315_MEDIA:\n",
    "    our_dict[award] = {\n",
    "        'nominees' : media_nominees_dict[award],\n",
    "        'winner' : [],\n",
    "        'presenters' : []\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "easy_comp(OFFICIAL_AWARDS_1315_MEDIA,true_dict,our_dict);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "# New"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "people_awards, media_awards = separate_awards(OFFICIAL_AWARDS_1315)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#media_tweet, all_potential_media, full_media_dict = associated_tasks(media_awards, data, \"media\", media, people) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "IOPub data rate exceeded.\n",
      "The notebook server will temporarily stop sending output\n",
      "to the client in order to avoid crashing it.\n",
      "To change this limit, set the config variable\n",
      "`--NotebookApp.iopub_data_rate_limit`.\n",
      "\n",
      "Current values:\n",
      "NotebookApp.iopub_data_rate_limit=1000000.0 (bytes/sec)\n",
      "NotebookApp.rate_limit_window=3.0 (secs)\n",
      "\n"
     ]
    },
    {
     "ename": "IndexError",
     "evalue": "list index out of range",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-91-7bed6a984e3a>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mpeople_tweet\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mall_potential_media\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfull_people_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpeople_presenters\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0massociated_tasks\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpeople_awards\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"people\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpeople\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpeople\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-88-6ac33ac1f3c5>\u001b[0m in \u001b[0;36massociated_tasks\u001b[0;34m(award_list, data, spec, kb, kb2)\u001b[0m\n\u001b[1;32m     27\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     28\u001b[0m     \u001b[0;31m# Get possible presenters\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 29\u001b[0;31m     \u001b[0mfull_presenters\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mget_presenters\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0maward_list\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcat_filter_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtweets_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfinal_winners\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     30\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     31\u001b[0m     \u001b[0;31m# Filter out all presenters not in the secondary kb\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-89-872cbbf19273>\u001b[0m in \u001b[0;36mget_presenters\u001b[0;34m(award_list, categories_dict, tweets_dict, winners)\u001b[0m\n\u001b[1;32m     11\u001b[0m         \u001b[0mall_presenters\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mextract_presenters\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtweets_dict\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0maward\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcategories_dict\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0maward\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mwinners\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0maward\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m         \u001b[0;31m#print(all_presenters)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 13\u001b[0;31m         \u001b[0mpresenters\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0maward\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m' '\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mall_presenters\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     14\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     15\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mpresenters\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mIndexError\u001b[0m: list index out of range"
     ]
    }
   ],
   "source": [
    "people_tweet, all_potential_media, full_people_dict, people_presenters = associated_tasks(people_awards, data, \"people\", people, people) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "people_presenters"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

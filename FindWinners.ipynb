{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 205,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    " \n",
    "import nltk\n",
    "from nltk.probability import FreqDist\n",
    "from nltk.tokenize import TweetTokenizer\n",
    "from nltk.tokenize import sent_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "import string\n",
    "import math\n",
    "import re\n",
    "from collections import Counter "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 206,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "data = pd.read_json('gg2013.json')\n",
    "data = pd.DataFrame(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 207,
   "metadata": {},
   "outputs": [],
   "source": [
    "# return cleaned Tweet as string\n",
    "# remove stopwords, user handles, punctuation, urls\n",
    "\n",
    "def cleanTweets(tweet):\n",
    "    tt = TweetTokenizer(strip_handles=True, reduce_len=True, preserve_case=True)\n",
    "\n",
    "    punctuation = list(string.punctuation)\n",
    "    \n",
    "    # strip stopwords, punctuation, url components \n",
    "    stop = stopwords.words('english') + punctuation + ['t.co', 'http', 'https', '...', '..', ':\\\\', 'RT', '#']\n",
    "\n",
    "    strip_nums = re.sub(\"\\d+\", \"\", tweet)\n",
    "    tokenized = tt.tokenize(strip_nums)\n",
    "    terms_stop = [term for term in tokenized if term not in stop]\n",
    "    cleaned = [term for term in terms_stop]\n",
    "    cleaned = ' '.join(cleaned)\n",
    "    \n",
    "    return cleaned"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 208,
   "metadata": {},
   "outputs": [],
   "source": [
    "def filter0(data):\n",
    "    results = []\n",
    "    \n",
    "    for tweet in data['text']:\n",
    "        if 'best' in tweet.lower():\n",
    "            results.append(tweet)\n",
    "            \n",
    "    return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 209,
   "metadata": {},
   "outputs": [],
   "source": [
    "f0 = data['text'].values.tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 210,
   "metadata": {},
   "outputs": [],
   "source": [
    "def filter1(data, list1, list2):\n",
    "    result1 = []\n",
    "    result2 = []\n",
    "    \n",
    "    for tweet in data:\n",
    "        if any(term in tweet.lower() for term in list1):\n",
    "            result1.append(tweet)\n",
    "        elif any(term in tweet.lower() for term in list2):\n",
    "            result2.append(tweet)\n",
    "            \n",
    "    return result1, result2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 211,
   "metadata": {},
   "outputs": [],
   "source": [
    "movie = ['motion picture', 'movie', 'movies']\n",
    "tv = ['tv', 'television']\n",
    "\n",
    "f1_movie, f1_tv = filter1(f0, movie, tv)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 212,
   "metadata": {},
   "outputs": [],
   "source": [
    "def filter2(data, list1):\n",
    "    result1 = []\n",
    "    result2 = []\n",
    "        \n",
    "    for tweet in data:\n",
    "        if any(term in tweet.lower() for term in list1):\n",
    "            result1.append(tweet)\n",
    "        else:\n",
    "            result2.append(tweet)\n",
    "            \n",
    "    return result1, result2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 213,
   "metadata": {},
   "outputs": [],
   "source": [
    "actor = ['actor', 'actress', 'actors', 'actresses']\n",
    "f2_movie_actor, f2_movie_media = filter2(f1_movie, actor)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 214,
   "metadata": {},
   "outputs": [],
   "source": [
    "f2_tv_actor, f2_tv_media = filter2(f1_tv, actor)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 215,
   "metadata": {},
   "outputs": [],
   "source": [
    "f3_tv_actor, f3_tv_actress = filter1(f2_tv_actor, ['actor', 'actors'], ['actress', 'actresses'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 216,
   "metadata": {},
   "outputs": [],
   "source": [
    "f3_movie_actor, f3_movie_actress = filter1(f2_movie_actor, ['actor', 'actors'], ['actress', 'actresses'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 223,
   "metadata": {},
   "outputs": [],
   "source": [
    "f4_lifetime_achievement = filter2(data, ['cecil', 'demille', 'lifetime', 'achievement'])[0]\n",
    "f4_tv_drama = filter2(f2_tv_media, ['drama'])[0]\n",
    "f4_tv_musical_comedy = filter2(f2_tv_media, ['musical', 'comedy'])[0]\n",
    "f4_tv_miniseries_film = filter2(f2_tv_media, ['film', 'miniseries'])[0]\n",
    "f4_tv_drama_actor = filter2(f3_tv_actor, ['drama'])[0]\n",
    "f4_tv_musical_comedy_actor = filter2(f3_tv_actor, ['musical', 'comedy'])[0]\n",
    "f4_tv_miniseries_film_actor = filter2(f3_tv_actor, ['miniseries', 'mini', 'film'])[0]\n",
    "f4_tv_drama_actress = filter2(f3_tv_actress, ['drama'])[0]\n",
    "f4_tv_musical_comedy_actress = filter2(f3_tv_actress, ['musical', 'comedy'])[0]\n",
    "f4_tv_miniseries_film_actress = filter2(f3_tv_actress, ['miniseries', 'mini', 'film'])[0]\n",
    "f4_tv_supporting_actor_series = filter2(f3_tv_actor, ['supporting', 'series', 'mini', 'miniseries', 'film'])[0]\n",
    "f4_tv_supporting_actress_series = filter2(f3_tv_actress, ['supporting', 'series', 'mini', 'miniseries', 'film'])[0]\n",
    "f4_achievement_in_tv = filter2(data, ['carol', 'burnett'])[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 224,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extractWinner(data, list1):\n",
    "    result = []\n",
    "    \n",
    "    tt = TweetTokenizer(strip_handles=True, reduce_len=True, preserve_case=True)\n",
    "\n",
    "    punctuation = list(string.punctuation)\n",
    "    remove_terms = ['#GoldenGlobes', 'Golden Globes', 'golden globes', '#GoldenGlobe', 'Golden Globe', '#goldenglobes']    \n",
    "    # strip stopwords, punctuation, url components \n",
    "    stop = stopwords.words('english') + punctuation + ['t.co', 'http', 'https', '...', '..', ':\\\\', 'rt', '#'] + remove_terms + list1\n",
    "\n",
    "    for tweet in data:\n",
    "        tweet = re.sub(\"\\d+\", \"\", tweet) #strip nums\n",
    "        tweet = re.sub(r'http\\S+', '', tweet)\n",
    "        tweet = tt.tokenize(tweet) #tokenize\n",
    "        tweet = [term for term in tweet if term.lower() not in stop] #remove stop words\n",
    "        result.append(' '.join(tweet))\n",
    "        \n",
    "    bgrams = [];\n",
    "\n",
    "    for tweet in result:\n",
    "        tweet = re.findall('([A-Z][a-z]+)', tweet)\n",
    "        if tweet:\n",
    "            bgrams += list(nltk.bigrams(tweet))\n",
    "        \n",
    "    fdist = nltk.FreqDist(bgrams)\n",
    "    \n",
    "    return fdist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 225,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[]"
      ]
     },
     "execution_count": 225,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "extractWinner(f4_lifetime_achievement, ['cecil', 'demille', 'lifetime', 'achievement']).most_common(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 226,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[]"
      ]
     },
     "execution_count": 226,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "extractWinner(f4_tv_drama, ['television', 'tv', 'series', 'drama']).most_common(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 227,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[]"
      ]
     },
     "execution_count": 227,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "extractWinner(f4_tv_musical_comedy, ['television', 'tv', 'series', 'musical', 'comedy']).most_common(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 222,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[]"
      ]
     },
     "execution_count": 222,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "extractWinner(f4_tv_miniseries_film, ['television', 'tv', 'film', 'miniseries']).most_common(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

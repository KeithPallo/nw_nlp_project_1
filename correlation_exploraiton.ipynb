{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 349,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import re \n",
    "\n",
    "import nltk;\n",
    "from nltk.collocations import *;\n",
    "from nltk.tokenize import word_tokenize;\n",
    "from nltk.corpus import stopwords;\n",
    "from nltk.probability import FreqDist\n",
    "from nltk.tokenize import TweetTokenizer\n",
    "from nltk import ngrams"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 350,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.options.mode.chained_assignment = None  # default='warn'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 351,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]     /Users/keithpallo/nltk_data...\n",
      "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
      "[nltk_data]       date!\n"
     ]
    }
   ],
   "source": [
    "nltk.download('averaged_perceptron_tagger');"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 352,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_13 = pd.read_json(\"gg2013.json\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 353,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_13 = pd.DataFrame(df_13)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 354,
   "metadata": {},
   "outputs": [],
   "source": [
    "OFFICIAL_AWARDS = ['cecil b. demille award', \n",
    "'best motion picture - drama', \n",
    "'best performance by an actress in a motion picture - drama', \n",
    "'best performance by an actor in a motion picture - drama', \n",
    "'best motion picture - comedy or musical', \n",
    " 'best performance by an actress in a motion picture - comedy or musical', \n",
    " 'best performance by an actor in a motion picture - comedy or musical', \n",
    " 'best animated feature film', 'best foreign language film', \n",
    " 'best performance by an actress in a supporting role in a motion picture', \n",
    " 'best performance by an actor in a supporting role in a motion picture', \n",
    " 'best director - motion picture', 'best screenplay - motion picture', \n",
    " 'best original score - motion picture', 'best original song - motion picture', \n",
    " 'best television series - drama', 'best performance by an actress in a television series - drama', \n",
    " 'best performance by an actor in a television series - drama', 'best television series - comedy or musical', \n",
    " 'best performance by an actress in a television series - comedy or musical', \n",
    " 'best performance by an actor in a television series - comedy or musical', \n",
    " 'best mini-series or motion picture made for television', \n",
    " 'best performance by an actress in a mini-series or motion picture made for television', \n",
    " 'best performance by an actor in a mini-series or motion picture made for television', \n",
    " 'best performance by an actress in a supporting role in a series, mini-series or motion picture made for television',\n",
    " 'best performance by an actor in a supporting role in a series, mini-series or motion picture made for television']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 355,
   "metadata": {},
   "outputs": [],
   "source": [
    "clean_awards = [re.sub(\"[^a-zA-Z0-9]+\", ' ',i) for i in OFFICIAL_AWARDS];"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 356,
   "metadata": {},
   "outputs": [],
   "source": [
    "pos_structure = []\n",
    "\n",
    "for sentence in clean_awards:\n",
    "    tokenized_text = nltk.word_tokenize(sentence)\n",
    "    tagged = nltk.pos_tag(tokenized_text)\n",
    "    pos_structure.append([i[1] for i in tagged])\n",
    "\n",
    "for i in range(0,len(clean_awards)):\n",
    "    pos_structure[i].append(clean_awards[i])\n",
    "    \n",
    "award_dict = {}\n",
    "\n",
    "for award in pos_structure:\n",
    "    award_dict[award[-1]] = award[:-1]\n",
    "    \n",
    "pos_structure.sort()\n",
    "# print(*pos_structure,sep='\\n')\n",
    "\n",
    "# print(award_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 360,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cleanTweets(tweet):\n",
    "    tt = TweetTokenizer(strip_handles=True, reduce_len=True, preserve_case=True)\n",
    "\n",
    "    punctuation = list(string.punctuation)\n",
    "    \n",
    "    # strip stopwords, punctuation, url components \n",
    "    stop = stopwords.words('english') + punctuation + ['t.co', 'http', 'https', '...', '..', ':\\\\', 'RT', '#']\n",
    "\n",
    "    strip_nums = re.sub(\"\\d+\", \"\", tweet)\n",
    "    tokenized = tt.tokenize(strip_nums)\n",
    "    terms_stop = [term for term in tokenized if term not in stop]\n",
    "    cleaned = [term for term in terms_stop]\n",
    "    cleaned = ' '.join(cleaned)\n",
    "    \n",
    "    return cleaned"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 361,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_awards(text):\n",
    "    \n",
    "    text = re.sub(\"(\\s)#\\w+\",\"\",text)    # strips away all hashtags \n",
    "    text = re.sub(\"[^a-zA-Z ]\", '',text) # removes all punctuation but keeps whitespace for tokenization\n",
    "    text = text.lower()                  # makes string lowercase\n",
    "     \n",
    "    return text "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 362,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_tags(tweet):\n",
    "    tokenizer = TweetTokenizer()\n",
    "    tokens = tokenizer.tokenize(tweet)\n",
    "    tags = nltk.pos_tag(tokens)\n",
    "    return tags"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 363,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pos_search(tags,chunk_gram,label):\n",
    "    potentials = \"No Chunk\"\n",
    "    chunk_parser = nltk.RegexpParser(chunk_gram)\n",
    "    chunked = chunk_parser.parse(tags)\n",
    "    for subtree in chunked.subtrees():\n",
    "        if subtree.label() == label: \n",
    "            potentials = ' '.join(untag(subtree))\n",
    "\n",
    "    return potentials"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 364,
   "metadata": {},
   "outputs": [],
   "source": [
    "def filter_df(df,label):\n",
    "    \n",
    "    data = df.loc[df[label] != \"No Chunk\"]\n",
    "    data.drop(data.columns.difference([label]), 1, inplace=True)\n",
    "    single_list = list(data[label])\n",
    "    freq = FreqDist(single_list)\n",
    "    \n",
    "    return data, freq"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 365,
   "metadata": {},
   "outputs": [],
   "source": [
    "def simple_test(dic,freq):\n",
    "    \n",
    "    test = []\n",
    "    \n",
    "    for key in dic.keys():\n",
    "        if key in freq:\n",
    "            test.append([key,freq[key]])\n",
    "        else:\n",
    "            test.append([key,\"Not found\"])\n",
    "        \n",
    "    print(*test,sep='\\n')   \n",
    "    return test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 366,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_13['text'] = df_13['text'].apply(lambda x:  clean_awards(x))\n",
    "df_a = df_13[df_13['text'].str.contains(\"best\")]\n",
    "df_a['tags'] = df_a['text'].apply(lambda x: find_tags(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 374,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "df_a['chunks_0'] = df_a['tags'].apply(lambda x: pos_search(x,regex_pattern_0,\"P0\"))\n",
    "df_a['chunks_1'] = df_a['tags'].apply(lambda x: pos_search(x,regex_pattern_1,\"P1\"))\n",
    "df_a['chunks_2'] = df_a['tags'].apply(lambda x: pos_search(x,regex_pattern_2,\"P2\"))\n",
    "\n",
    "data_0, freq_0 = filter_df(df_a,\"chunks_0\")\n",
    "data_1, freq_1 = filter_df(df_a,\"chunks_1\")\n",
    "data_2, freq_2 = filter_df(df_a,\"chunks_2\")\n",
    "\n",
    "freq = freq_0 + freq_1 + freq_2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 370,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "False\n"
     ]
    }
   ],
   "source": [
    "# specific test bloc \n",
    "\n",
    "regex_pattern_test = \"test: {<RBS><NN><IN><DT><NN><IN><DT><NN><NN><NN><CC><JJ>}\"\n",
    "\n",
    "df_a['test'] = df_a['tags'].apply(lambda x: pos_search(x,regex_pattern_test,\"test\"))\n",
    "\n",
    "data_test, freq_test = filter_df(df_a,\"test\")\n",
    "\n",
    "print('best performance by an actress in a television series comedy or musical' in freq_test)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Preprocessing Start"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 213,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     /Users/keithpallo/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "import nltk\n",
    "from nltk.probability import FreqDist\n",
    "from nltk.tokenize import TweetTokenizer\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.corpus import wordnet\n",
    "nltk.download('wordnet')\n",
    "\n",
    "from collections import Counter\n",
    "from collections import OrderedDict\n",
    "\n",
    "import string\n",
    "import re\n",
    "import unidecode\n",
    "import requests\n",
    "import json\n",
    "from pprint import pprint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load in official award names\n",
    "\n",
    "OFFICIAL_AWARDS_1315_media = ['best motion picture - drama', 'best motion picture - comedy or musical', 'best animated feature film', 'best foreign language film', 'best screenplay - motion picture', 'best original score - motion picture', 'best original song - motion picture', 'best television series - drama', 'best television series - comedy or musical', 'best mini-series or motion picture made for television']\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load in answers - we will remove this when we submit the final fial\n",
    "\n",
    "with open('gg2015answers.json') as f:\n",
    "    answers = json.load(f)\n",
    "    \n",
    "true_dict = answers['award_data'] # use this for true dict in the below function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loadin dataset - TO DO, automate this processes for four relevant years (2013,2015,2018,2019)\n",
    "\n",
    "data = pd.read_json('../gg2015.json')\n",
    "data = pd.DataFrame(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Write dataset to a list\n",
    "\n",
    "data = data['text'].values.tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize seperate knowledge bases \n",
    "\n",
    "people = set()\n",
    "media = set() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "url = 'https://query.wikidata.org/sparql'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "query = \"\"\"\n",
    "# ALL PERSONS required for awards\n",
    "SELECT DISTINCT ?person ?personLabel WHERE {\n",
    "# FIRST: uncomment occupation:\n",
    "  ?person wdt:P31 wd:Q5;\n",
    "           wdt:P106/wdt:P279* wd:Q2526255; #uncomment for     FILM director (no award for TV director)\n",
    "  FILTER NOT EXISTS { ?person wdt:P570 ?date. } #person is alive\n",
    "  \n",
    "# SECOND: uncomment gender if applicable (for actor/actress):\n",
    "#          wdt:P21 wd:Q6581097;    #male\n",
    "#          wdt:P21 wd:Q6581072;    #female\n",
    "  SERVICE wikibase:label { bd:serviceParam wikibase:language \"[AUTO_LANGUAGE],en\". }\n",
    "}\n",
    "\"\"\"\n",
    "r = requests.get(url, params = {'format': 'json', 'query': query})\n",
    "kb = r.json()\n",
    "for item in kb['results']['bindings']:\n",
    "    people.add(unidecode.unidecode(item['personLabel']['value']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "query = \"\"\"\n",
    "# ALL PERSONS required for awards\n",
    "SELECT DISTINCT ?person ?personLabel WHERE {\n",
    "# FIRST: uncomment occupation:\n",
    "  ?person wdt:P31 wd:Q5;\n",
    "           wdt:P106/wdt:P279* wd:Q10800557; #uncomment for    FILM actor (don't just use actor)\n",
    "  FILTER NOT EXISTS { ?person wdt:P570 ?date. } #person is alive\n",
    "  \n",
    "# SECOND: uncomment gender if applicable (for actor/actress):\n",
    "#          wdt:P21 wd:Q6581097;    #male\n",
    "#          wdt:P21 wd:Q6581072;    #female\n",
    "  SERVICE wikibase:label { bd:serviceParam wikibase:language \"[AUTO_LANGUAGE],en\". }\n",
    "}\n",
    "\"\"\"\n",
    "r1 = requests.get(url, params = {'format': 'json', 'query': query})\n",
    "kb1 = r1.json()\n",
    "for item in kb1['results']['bindings']:\n",
    "    people.add(unidecode.unidecode(item['personLabel']['value']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "query = \"\"\"\n",
    "# ALL PERSONS required for awards\n",
    "SELECT DISTINCT ?person ?personLabel WHERE {\n",
    "# FIRST: uncomment occupation:\n",
    "  ?person wdt:P31 wd:Q5;\n",
    "           wdt:P106/wdt:P279* wd:Q10798782; #uncomment for    TV actor (don't just use actor)\n",
    "  FILTER NOT EXISTS { ?person wdt:P570 ?date. } #person is alive\n",
    "  \n",
    "# SECOND: uncomment gender if applicable (for actor/actress):\n",
    "#          wdt:P21 wd:Q6581097;    #male\n",
    "#          wdt:P21 wd:Q6581072;    #female\n",
    "  SERVICE wikibase:label { bd:serviceParam wikibase:language \"[AUTO_LANGUAGE],en\". }\n",
    "}\n",
    "\"\"\"\n",
    "r = requests.get(url, params = {'format': 'json', 'query': query})\n",
    "kb = r.json()\n",
    "for item in kb['results']['bindings']:\n",
    "    people.add(unidecode.unidecode(item['personLabel']['value']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "query = \"\"\"\n",
    "# ALL PERSONS required for awards\n",
    "SELECT DISTINCT ?person ?personLabel WHERE {\n",
    "# FIRST: uncomment occupation:\n",
    "  ?person wdt:P31 wd:Q5;\n",
    "           wdt:P106/wdt:P279* wd:Q36834; #uncomment for       composer (cannot use songwriter)\n",
    "  FILTER NOT EXISTS { ?person wdt:P570 ?date. } #person is alive\n",
    "  \n",
    "# SECOND: uncomment gender if applicable (for actor/actress):\n",
    "#          wdt:P21 wd:Q6581097;    #male\n",
    "#          wdt:P21 wd:Q6581072;    #female\n",
    "  SERVICE wikibase:label { bd:serviceParam wikibase:language \"[AUTO_LANGUAGE],en\". }\n",
    "}\n",
    "\"\"\"\n",
    "r = requests.get(url, params = {'format': 'json', 'query': query})\n",
    "kb = r.json()\n",
    "for item in kb['results']['bindings']:\n",
    "    people.add(unidecode.unidecode(item['personLabel']['value']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "query = \"\"\"\n",
    "# ALL PERSONS required for awards\n",
    "SELECT DISTINCT ?person ?personLabel WHERE {\n",
    "# FIRST: uncomment occupation:\n",
    "  ?person wdt:P31 wd:Q5;\n",
    "           wdt:P106/wdt:P279* wd:Q28389; #uncomment for       screenwriter\n",
    "  FILTER NOT EXISTS { ?person wdt:P570 ?date. } #person is alive\n",
    "  \n",
    "# SECOND: uncomment gender if applicable (for actor/actress):\n",
    "#          wdt:P21 wd:Q6581097;    #male\n",
    "#          wdt:P21 wd:Q6581072;    #female\n",
    "  SERVICE wikibase:label { bd:serviceParam wikibase:language \"[AUTO_LANGUAGE],en\". }\n",
    "}\n",
    "\"\"\"\n",
    "r = requests.get(url, params = {'format': 'json', 'query': query})\n",
    "kb = r.json()\n",
    "for item in kb['results']['bindings']:\n",
    "    people.add(unidecode.unidecode(item['personLabel']['value']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 232,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Media KB Creation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "metadata": {},
   "outputs": [],
   "source": [
    "url = 'https://query.wikidata.org/sparql'\n",
    "query = \"\"\"\n",
    "SELECT DISTINCT ?itemLabel  WHERE {\n",
    " ?item wdt:P31 wd:Q11424. ?item wdt:P577 ?_publication_date. ?item wdt:P136 ?_genre.\n",
    " ?_genre rdfs:label ?_genreLabel. BIND(str(YEAR(?_publication_date)) AS ?year)\n",
    " FILTER((LANG(?_genreLabel)) = \"en\")\n",
    " FILTER (?_publication_date >= \"2010-00-00T00:00:00Z\"^^xsd:dateTime && ?_publication_date <= \"2019-00-00T00:00:00Z\"^^xsd:dateTime )\n",
    " SERVICE wikibase:label { bd:serviceParam wikibase:language \"[AUTO_LANGUAGE],en\" .} }\n",
    "\"\"\"\n",
    "r = requests.get(url, params = {'format': 'json', 'query': query})\n",
    "kb_m = r.json()\n",
    "\n",
    "for item in kb_m['results']['bindings']:\n",
    "    media.add(unidecode.unidecode(item['itemLabel']['value']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "metadata": {},
   "outputs": [],
   "source": [
    "query = \"\"\"\n",
    "SELECT DISTINCT ?itemLabel  WHERE {\n",
    "  ?item wdt:P31 wd:Q5398426.\n",
    "  ?item wdt:P580  ?_start\n",
    " FILTER (?_start >= \"2000-00-00T00:00:00Z\"^^xsd:dateTime && ?_start <= \"2019-00-00T00:00:00Z\"^^xsd:dateTime )\n",
    "  SERVICE wikibase:label {bd:serviceParam wikibase:language \"[AUTO_LANGUAGE],en\" .}\n",
    "}\n",
    "    \n",
    "\"\"\"\n",
    "r = requests.get(url, params = {'format': 'json', 'query': query})\n",
    "kb_m = r.json()\n",
    "\n",
    "for item in kb_m['results']['bindings']:\n",
    "    media.add(unidecode.unidecode(item['itemLabel']['value']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "f = open('english.txt', 'r')\n",
    "stop_words = f.read().splitlines()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "OFFICIAL_AWARDS_1315_people = ['cecil b. demille award',\n",
    "                        'best performance by an actress in a motion picture - drama', \n",
    "                        'best performance by an actor in a motion picture - drama',\n",
    "                        'best performance by an actress in a motion picture - comedy or musical', \n",
    "                        'best performance by an actor in a motion picture - comedy or musical',\n",
    "                        'best performance by an actress in a supporting role in a motion picture', \n",
    "                        'best performance by an actor in a supporting role in a motion picture', \n",
    "                        'best director - motion picture', \n",
    "                        'best screenplay - motion picture', \n",
    "                        'best original score - motion picture',\n",
    "                        'best performance by an actress in a television series - drama', \n",
    "                        'best performance by an actor in a television series - drama',\n",
    "                        'best performance by an actress in a television series - comedy or musical', \n",
    "                        'best performance by an actor in a television series - comedy or musical', \n",
    "                        'best performance by an actress in a mini-series or motion picture made for television', \n",
    "                        'best performance by an actor in a mini-series or motion picture made for television', \n",
    "                        'best performance by an actress in a supporting role in a series, mini-series or motion picture made for television', \n",
    "                        'best performance by an actor in a supporting role in a series, mini-series or motion picture made for television']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Preprocessing Complete\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "def parseAward(award):\n",
    "    \"\"\"\n",
    "    Returns a list of words that can be used to filter for a particular award\n",
    "    \"\"\"\n",
    "    \n",
    "    award = re.split('\\W+', award)\n",
    "    award = [i for i in award if i not in stop_words]\n",
    "    award = list(set(award))\n",
    "    return award"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getCategoriesDict(awards_list):\n",
    "    \"\"\"\n",
    "    Returns a dictionary that has all awards as keys, and a list of relevant filtering words as values\n",
    "    \"\"\"\n",
    "    \n",
    "    categories_dict = dict()\n",
    "    for a in awards_list:\n",
    "        terms = parseAward(a)\n",
    "        categories_dict[a] = terms\n",
    "\n",
    "    return categories_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 166,
   "metadata": {},
   "outputs": [],
   "source": [
    "def filter0(data, list1, spec = \"people\"):\n",
    "    \"\"\"\n",
    "    Returns a list of tweets that are relevant to a particular award\n",
    "    \"\"\"\n",
    "    synonyms = {}\n",
    "    \n",
    "    if spec == \"people\":\n",
    "        synonyms = {\n",
    "            'motion' : ['motion picture', 'motion', 'picture', 'movie'],\n",
    "            'picture' : ['motion picture', 'motion', 'picture', 'movie'],\n",
    "            'television' : ['television', 'tv'],\n",
    "            'mini' : ['mini-series', 'mini', 'series', 'miniseries'],\n",
    "            'series' : ['mini-series', 'mini', 'series', 'miniseries']\n",
    "        }\n",
    "\n",
    "    \n",
    "    result = []\n",
    "    \n",
    "    list1 = [i for i in list1 if i != 'performance' and i != 'role']\n",
    "\n",
    "    for tweet in data:\n",
    "        cond = True\n",
    "        for i in list1:\n",
    "            if i in synonyms:\n",
    "                if all(j not in tweet.lower() for j in synonyms[i]):\n",
    "                    cond = False\n",
    "            elif i not in tweet.lower():\n",
    "                cond = False\n",
    "        if cond:\n",
    "            result.append(tweet)\n",
    "\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def extractPeople(data, list1):\n",
    "    \"\"\"\n",
    "    Extracts potential People nominees from an individual tweet\n",
    "    \"\"\"\n",
    "    \n",
    "    \n",
    "    result = []\n",
    "       \n",
    "    translator = str.maketrans('', '', string.punctuation)\n",
    "    remove_terms = ['#goldenglobes', 'golden globes', '#goldenglobe', 'golden globe', 'goldenglobes', 'goldenglobe', 'rt', 'golden', 'globe', 'globes']\n",
    "    stop = remove_terms + list1\n",
    "    \n",
    "    for tweet in data:\n",
    "        \n",
    "        tweet = re.sub(\"\\d+\", \"\", tweet)       #strip nums\n",
    "        tweet = re.sub(r'http\\S+', '', tweet)  #strip urls\n",
    "        tweet = re.sub(r'#\\S+', '', tweet)     #strip hashtags\n",
    "        tweet = tweet.translate(translator)    #strip non-alphanumeric characters\n",
    "        tweet = tweet.split()                  #tokenize\n",
    "        tweet = [term for term in tweet if term.lower() not in stop_words] #remove stop words\n",
    "        for i in stop:\n",
    "            for j in tweet:\n",
    "                if i.lower() in j.lower():\n",
    "                    tweet.remove(j)\n",
    "        result.append(tweet)\n",
    "        \n",
    "\n",
    "        \n",
    "    grams = [];\n",
    "\n",
    "    for tweet in result:\n",
    "        if tweet:\n",
    "            # Get all possible bigrams & trigrams in a tweet\n",
    "            gram = list(nltk.everygrams(tweet, 2, 3))\n",
    "            \n",
    "            # Filter through and append to list for tweet\n",
    "            for g in gram:\n",
    "                if len(g) == 2:\n",
    "                    if bool(re.match(r'\\b[A-Z][a-z]+\\b', g[0])) and bool(re.match(r'\\b[A-Z][a-z]+\\b', g[1])):\n",
    "                        grams.append(g)\n",
    "                else:\n",
    "                    if bool(re.match(r'\\b[A-Z][a-z]+\\b', g[0])) and bool(re.match(r'\\b[A-Z][a-z]+\\b', g[1])) and bool(re.match(r'\\b[A-Z][a-z]+\\b', g[2])):\n",
    "                        grams.append(g)\n",
    "\n",
    "  \n",
    "    fdist = nltk.FreqDist(grams)\n",
    "\n",
    "    try:\n",
    "        names = fdist.most_common()\n",
    "    except:\n",
    "        names = \"nothing here\"\n",
    "    \n",
    "    return names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 223,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extractMedia(data, list1):\n",
    "    \"\"\"\n",
    "    Extracts potential media nominees from an individual tweet\n",
    "    \"\"\"\n",
    "    \n",
    "    result = []\n",
    "       \n",
    "    translator = str.maketrans('', '', string.punctuation)\n",
    "    remove_terms = ['#goldenglobes', 'golden globes', '#goldenglobe', 'golden globe', 'goldenglobes', 'goldenglobe', 'rt', 'golden', 'globe', 'globes', 'best']    \n",
    "    stop = remove_terms + list1\n",
    "    \n",
    "    for tweet in data:\n",
    "        tweet = re.sub(\"\\d+\", \"\", tweet)      #strip nums\n",
    "        tweet = re.sub(r'http\\S+', '', tweet) #strip urls\n",
    "        tweet = re.sub(r'#\\S+', '', tweet)    #strip hashtags\n",
    "        tweet = tweet.translate(translator)   #strip non-alphanumeric characters\n",
    "        tweet = tweet.split()                 #tokenize\n",
    "\n",
    "        for i in stop:\n",
    "            for j in tweet:\n",
    "                if i.lower() in j.lower():\n",
    "                    tweet.remove(j)\n",
    "        tweet = ' '.join(tweet)\n",
    "        result.append(tweet)\n",
    "        \n",
    "        \n",
    "    grams = [];\n",
    "\n",
    "    for tweet in result:\n",
    "        if tweet:\n",
    "            \n",
    "            grams.extend(re.findall(r\"([A-Z][\\w-]*(?:\\s+[A-Z][\\w-]*)+)\", tweet))\n",
    "            grams.extend(re.findall(r\"\\b[A-Z][a-z]+\\b.*\\b[A-Z][a-z]+\\b\", tweet))\n",
    "            #singular = re.findall(r\"\\b[A-Z][a-z]+\\b\", tweet)\n",
    "            #singular = [i for i in singular if not wordnet.synsets(i)]\n",
    "            #grams.extend(singular)\n",
    "            \n",
    "  \n",
    "    fdist = nltk.FreqDist(grams)\n",
    "\n",
    "    try:\n",
    "        names = fdist.most_common()\n",
    "\n",
    "    except:\n",
    "        names = \"nothing here\"\n",
    "    \n",
    "    return names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_category_tweets(award_list, categories_dict,data):\n",
    "    \"\"\"\n",
    "    Using an award list and category dictionary, filters out tweets at an award level\n",
    "    \"\"\"\n",
    "    \n",
    "    d = {}\n",
    "    for award in award_list:\n",
    "        d[\"{0}\".format(award)] = filter0(data, categories_dict[award])\n",
    "                                         \n",
    "    return d"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 240,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_nom(award_list, categories_dict, tweets_dict, spec = \"\"):\n",
    "    \"\"\"\n",
    "    Gets all potential nominees based on ExtractMedia or ExtractPeople\n",
    "    \"\"\"\n",
    "    \n",
    "    if spec == \"people\":\n",
    "        funct = extractMedia\n",
    "    elif spec == \"media\":\n",
    "        funct = extractPeople\n",
    "    else:\n",
    "        print(\"there is a problem\")\n",
    "\n",
    "    nominees = {}\n",
    "    for award in award_list:\n",
    "        nominees[\"{0}\".format(award)] = funct(tweets_dict[award], categories_dict[award])\n",
    "    \n",
    "    return nominees"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compareToKB(nominees,kb):\n",
    "    \"\"\"\n",
    "    Takes in a dictionary of potential nominees and removes those that don't appear in a relevant KB\n",
    "    \n",
    "    If no nominees are in the KB, then ... (currentlt top five)\n",
    "    \"\"\"\n",
    "    \n",
    "    final_nominees = {}\n",
    "        \n",
    "    for i in nominees:\n",
    "        award_nominees = []\n",
    "        \n",
    "        for j in nominees[i]:\n",
    "            if j[0] in kb:\n",
    "                award_nominees.append(j[0])\n",
    "                \n",
    "        if not award_nominees:\n",
    "            award_nominees = [i[0] for i in nominees[i][:5]]\n",
    "        \n",
    "        final_nominees[i] = award_nominees\n",
    "        \n",
    "    return final_nominees"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 246,
   "metadata": {},
   "outputs": [],
   "source": [
    "def separateAwards(award_list):\n",
    "    possible_people_awards = ['actor',' actress', 'musician', ' singer', 'composer', 'director', 'producer',\n",
    "                        'screenwriter', 'stage technician', 'author']\n",
    "\n",
    "    people_awards = []\n",
    "    media_awards = []\n",
    "\n",
    "    for category in award_list:\n",
    "        if any(job in category.lower() for job in possible_people_awards):\n",
    "            people_awards.append(category)\n",
    "        else:\n",
    "            media_awards.append(category)\n",
    "\n",
    "    return people_awards, media_awards"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compress_associated_dict(award_list,nominees,winners,presenters):\n",
    "    \n",
    "    our_dict = {}\n",
    "\n",
    "    for award in award_list:\n",
    "        our_dict[award] = {\n",
    "            'nominees' : nominees[award],\n",
    "            'winner' : winners[award],\n",
    "            'presenters' : presenters[award]\n",
    "        }\n",
    "        \n",
    "    return our_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## To DO\n",
    "\n",
    "def getMediaWinners(nominees):\n",
    "    pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 169,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getPeopleWinners(nominees):\n",
    "    final_winners = {}\n",
    "    \n",
    "    for award in nominees:\n",
    "        winner = ' '.join(nominees[award][0][0])\n",
    "        final_winners[award] = winner\n",
    "        \n",
    "    return final_winners"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def associated_tasks(award_list,data,spec,kb,kb2):\n",
    "    \n",
    "    # Create a dictionary to filter tweets at a category level\n",
    "    cat_filter_dict = getCategoriesDict(award_list)                 \n",
    "    \n",
    "    # Get all assocaited tweets for each award \n",
    "    tweets_dict = get_category_tweets(award_list, cat_filter_dict,data)\n",
    "    \n",
    "    # For each award, get all assocaited nominees\n",
    "    full_nom_dict = get_nom(award_list, cat_dict, tweets, spec)\n",
    "    \n",
    "    # Filter out all nominees that are not in the dictionary\n",
    "    final_nom = comparetoKB(full_nom_dict,kb)\n",
    "    \n",
    "    \n",
    "    # TO DO - Fill in the below functions\n",
    "    \n",
    "    if spec == \"media\":\n",
    "        pass \n",
    "        # final_winners = get_media_winners()\n",
    "    \n",
    "    elif spec ==\"people\":\n",
    "        final_winners = getPeopleWinners(full_nom_dict)\n",
    "    \n",
    "    # TO DO - \n",
    "    # Get presenters\n",
    "    # full_presenters = get_pres(award_list,cat_dict,tweets)\n",
    "    \n",
    "    # Filter out all presenters not in the secondary kb\n",
    "    # final_pres = comparetoKB(presenters,kb2)\n",
    "    \n",
    "    compressed_dict = compress_associated_dict(award_list,final_nom,final_winners,final_pres)\n",
    "    \n",
    "    # This return is only required for main_exec\n",
    "    return tweets_dict, full_nom_dict, compressed_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def main_exec(data,kb_p,kb_m):\n",
    "    \"\"\"\n",
    "    Main execution file - how you run the program\n",
    "    Itype: kb_p and kb_m are sets for our built KB's\n",
    "    \"\"\"\n",
    "    \n",
    "    # Call host search function\n",
    "    # // To Do // - Insert function call (should write to JSON / return a variable to write later )\n",
    "    \n",
    "    # Call award entity recognition function\n",
    "    # // To Do // - Insert function call (should write to JSON / return a variable to write later)\n",
    "    \n",
    "    # Set a variable to the hardcoded list\n",
    "    hardcoded = OFFICIAL_AWARDS_1315_media\n",
    "    \n",
    "    # Segment out awards award categories\n",
    "    people_awards, media_awards = seperateAwards(hardcoded)\n",
    "    \n",
    "    \n",
    "    ## Functions below need to return a dictionary with following structure\n",
    "    ## Key 1: Award Name, Value: Dictionary\n",
    "    ## Key 2: [ Nominees, Winners, Presenters] \n",
    "    \n",
    "    \n",
    "    # Call people award search function - winner, nominee, presenter (potentially swap last two)\n",
    "    \n",
    "    people_tweet, all_potential_people, full_people_dict = associated_tasks(people_awards,data,\"people\",kb_p,kb_p)\n",
    "    \n",
    "    media_tweet, all_potential_media, full_media_dict = associated_tasks(media_awards,\"media\",kb_m,kb_p)\n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    # Call people award search function - winner, nominee, presenter (potentially swap last two)\n",
    "    ## // To DO // - Insert function call ( only write to easy_comp now)\n",
    "    \n",
    "    \n",
    "    # Merge dictionaries from two above functions\n",
    "    # Return single dict for easy_comp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 163,
   "metadata": {},
   "outputs": [],
   "source": [
    "def easy_comp(award_list,true_dict,our_dict):\n",
    "        \n",
    "    # Input \n",
    "    # Dict of dictionaries \n",
    "    # Keys - Award Name\n",
    "    # Values - Dictionary with keys (nominees, presenters, winner)\n",
    "    \n",
    "    # Output\n",
    "    # Nested List (some elements are dictionarys)\n",
    "\n",
    "    output = []\n",
    "\n",
    "    for award in award_list:\n",
    "        output.append([award,[\"Guess\",our_dict[award]],[[\"True\",true_dict[award]]]])\n",
    "\n",
    "    \n",
    "    return output\n",
    "\n",
    "# For easy printing use pprint. It works nicely for updated comparison"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "people_categories_dict = getCategoriesDict(OFFICIAL_AWARDS_1315_people)\n",
    "media_categories_dict = getCategoriesDict(OFFICIAL_AWARDS_1315_media)\n",
    "media_categories_dict;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 167,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "22.43589629099006\n"
     ]
    }
   ],
   "source": [
    "import timeit\n",
    "start_time = timeit.default_timer()\n",
    "tweets = get_category_tweets(OFFICIAL_AWARDS_1315_media, media_categories_dict,data)\n",
    "elapsed = timeit.default_timer() - start_time\n",
    "print(elapsed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 242,
   "metadata": {},
   "outputs": [],
   "source": [
    "media_nominees = get_media_nom(OFFICIAL_AWARDS_1315_media, media_categories_dict, tweets, \"media\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 227,
   "metadata": {},
   "outputs": [],
   "source": [
    "media_nominees_dict = compareToKB(media_nominees, media)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 230,
   "metadata": {},
   "outputs": [],
   "source": [
    "our_dict = {}\n",
    "\n",
    "for award in OFFICIAL_AWARDS_1315_media:\n",
    "    our_dict[award] = {\n",
    "        'nominees' : media_nominees_dict[award],\n",
    "        'winner' : [],\n",
    "        'presenters' : []\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 239,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "easy_comp(OFFICIAL_AWARDS_1315_media,true_dict,our_dict);"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

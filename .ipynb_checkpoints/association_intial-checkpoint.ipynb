{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import re \n",
    "\n",
    "import nltk;\n",
    "from nltk.collocations import *;\n",
    "from nltk.tokenize import word_tokenize;\n",
    "from nltk.corpus import stopwords;\n",
    "from nltk.probability import FreqDist\n",
    "from nltk.tokenize import TweetTokenizer\n",
    "from nltk import ngrams"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.options.mode.chained_assignment = None  # default='warn'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]     C:\\Users\\keith\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
      "[nltk_data]       date!\n"
     ]
    }
   ],
   "source": [
    "nltk.download('averaged_perceptron_tagger');"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_13 = pd.read_json(\"gg2013.json\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_13 = pd.DataFrame(df_13)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "OFFICIAL_AWARDS = ['cecil b. demille award', \n",
    "'best motion picture - drama', \n",
    "'best performance by an actress in a motion picture - drama', \n",
    "'best performance by an actor in a motion picture - drama', \n",
    "'best motion picture - comedy or musical', \n",
    " 'best performance by an actress in a motion picture - comedy or musical', \n",
    " 'best performance by an actor in a motion picture - comedy or musical', \n",
    " 'best animated feature film', 'best foreign language film', \n",
    " 'best performance by an actress in a supporting role in a motion picture', \n",
    " 'best performance by an actor in a supporting role in a motion picture', \n",
    " 'best director - motion picture', 'best screenplay - motion picture', \n",
    " 'best original score - motion picture', 'best original song - motion picture', \n",
    " 'best television series - drama', 'best performance by an actress in a television series - drama', \n",
    " 'best performance by an actor in a television series - drama', 'best television series - comedy or musical', \n",
    " 'best performance by an actress in a television series - comedy or musical', \n",
    " 'best performance by an actor in a television series - comedy or musical', \n",
    " 'best mini-series or motion picture made for television', \n",
    " 'best performance by an actress in a mini-series or motion picture made for television', \n",
    " 'best performance by an actor in a mini-series or motion picture made for television', \n",
    " 'best performance by an actress in a supporting role in a series, mini-series or motion picture made for television',\n",
    " 'best performance by an actor in a supporting role in a series, mini-series or motion picture made for television']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "clean_awards = [re.sub(\"[^a-zA-Z0-9]+\", ' ',i) for i in OFFICIAL_AWARDS];"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "pos_structure = []\n",
    "\n",
    "for sentence in clean_awards:\n",
    "    tokenized_text = nltk.word_tokenize(sentence)\n",
    "    tagged = nltk.pos_tag(tokenized_text)\n",
    "    pos_structure.append([i[1] for i in tagged])\n",
    "\n",
    "for i in range(0,len(clean_awards)):\n",
    "    pos_structure[i].append(clean_awards[i])\n",
    "    \n",
    "award_dict = {}\n",
    "\n",
    "for award in pos_structure:\n",
    "    award_dict[award[-1]] = award[:-1]\n",
    "    \n",
    "pos_structure.sort()\n",
    "# print(*pos_structure,sep='\\n')\n",
    "\n",
    "# print(award_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 360,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cleanTweets(tweet):\n",
    "    tt = TweetTokenizer(strip_handles=True, reduce_len=True, preserve_case=True)\n",
    "\n",
    "    punctuation = list(string.punctuation)\n",
    "    \n",
    "    # strip stopwords, punctuation, url components \n",
    "    stop = stopwords.words('english') + punctuation + ['t.co', 'http', 'https', '...', '..', ':\\\\', 'RT', '#']\n",
    "\n",
    "    strip_nums = re.sub(\"\\d+\", \"\", tweet)\n",
    "    tokenized = tt.tokenize(strip_nums)\n",
    "    terms_stop = [term for term in tokenized if term not in stop]\n",
    "    cleaned = [term for term in terms_stop]\n",
    "    cleaned = ' '.join(cleaned)\n",
    "    \n",
    "    return cleaned"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 361,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_awards(text):\n",
    "    \n",
    "    text = re.sub(\"(\\s)#\\w+\",\"\",text)    # strips away all hashtags \n",
    "    text = re.sub(\"[^a-zA-Z ]\", '',text) # removes all punctuation but keeps whitespace for tokenization\n",
    "    text = text.lower()                  # makes string lowercase\n",
    "     \n",
    "    return text "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_entity(text):\n",
    "    text = re.sub(\"(\\s)#\\w+\",\"\",text)    # strips away all hashtags \n",
    "    text = re.sub(\"[^a-zA-Z ]\", '',text) # removes all punctuation but keeps whitespace for tokenization\n",
    "     \n",
    "    return text "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_tags(tweet):\n",
    "    tokenizer = TweetTokenizer()\n",
    "    tokens = tokenizer.tokenize(tweet)\n",
    "    tags = nltk.pos_tag(tokens)\n",
    "    return tags"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 363,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pos_search(tags,chunk_gram,label):\n",
    "    potentials = \"No Chunk\"\n",
    "    chunk_parser = nltk.RegexpParser(chunk_gram)\n",
    "    chunked = chunk_parser.parse(tags)\n",
    "    for subtree in chunked.subtrees():\n",
    "        if subtree.label() == label: \n",
    "            potentials = ' '.join(untag(subtree))\n",
    "\n",
    "    return potentials"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ne_search(tags):\n",
    "    potentials = \"No Entity\"\n",
    "    namedEnt = nltk.ne_chunk(tags)\n",
    "    \n",
    "    return namedEnt\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 364,
   "metadata": {},
   "outputs": [],
   "source": [
    "def filter_df(df,label):\n",
    "    \n",
    "    data = df.loc[df[label] != \"No Chunk\"]\n",
    "    data.drop(data.columns.difference([label]), 1, inplace=True)\n",
    "    single_list = list(data[label])\n",
    "    freq = FreqDist(single_list)\n",
    "    \n",
    "    return data, freq"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 365,
   "metadata": {},
   "outputs": [],
   "source": [
    "def simple_test(dic,freq):\n",
    "    \n",
    "    test = []\n",
    "    \n",
    "    for key in dic.keys():\n",
    "        if key in freq:\n",
    "            test.append([key,freq[key]])\n",
    "        else:\n",
    "            test.append([key,\"Not found\"])\n",
    "        \n",
    "    print(*test,sep='\\n')   \n",
    "    return test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_13['text'] = df_13['text'].apply(lambda x:  clean_entity(x))\n",
    "df_e = df_13[df_13['text'].str.contains(\"best\")]\n",
    "df_e['tags'] = df_e['text'].apply(lambda x: find_tags(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package maxent_ne_chunker to\n",
      "[nltk_data]     C:\\Users\\keith\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package maxent_ne_chunker is already up-to-date!\n",
      "[nltk_data] Downloading package words to\n",
      "[nltk_data]     C:\\Users\\keith\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Unzipping corpora\\words.zip.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.download('maxent_ne_chunker')\n",
    "nltk.download('words')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "df_e['entity'] = df_e['tags'].apply(lambda x: ne_search(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "16     [(Why, WRB), (did, VBD), (I, PRP), (just, RB),...\n",
       "340    [(RT, NNP), (ZapitRick, NNP), (Can, NNP), [(Ke...\n",
       "546    [(the, DT), (best, JJS), (phrase, NN), (I, PRP...\n",
       "694    [[(Nicole, NNP)], [(Kidman, NNP)], (is, VBZ), ...\n",
       "756    [(RT, NNP), (MovieMayor, NNP), (My, NNP), (gre...\n",
       "Name: entity, dtype: object"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_e['entity'].head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'spacy'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-22-86b5dfc9b16e>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[1;32mimport\u001b[0m \u001b[0mspacy\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      2\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0mspacy\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mdisplacy\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0mcollections\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mCounter\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0men_core_web_sm\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[0mnlp\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0men_core_web_sm\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mload\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'spacy'"
     ]
    }
   ],
   "source": [
    "import spacy\n",
    "from spacy import displacy\n",
    "from collections import Counter\n",
    "import en_core_web_sm\n",
    "nlp = en_core_web_sm.load()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
